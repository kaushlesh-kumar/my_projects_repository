{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kaushlesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports the libraries and read the data files\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import os, sys, email\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pprint\n",
    "import nltk\n",
    "from string import punctuation\n",
    "import timeit\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "start = timeit.default_timer()\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_df = pd.read_csv('./news_tagged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News_Heading</th>\n",
       "      <th>Risk Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMF chief Lagarde charged over French corrupt...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$1 Billion Agreement with SunTrust to Address ...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$1 million for GM victims' families</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$1 million IRS phone scam 'largest ever'</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$1.2 B settlement with Toyota because of safet...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        News_Heading Risk Tag\n",
       "0   IMF chief Lagarde charged over French corrupt...        F\n",
       "1  $1 Billion Agreement with SunTrust to Address ...        L\n",
       "2                $1 million for GM victims' families        L\n",
       "3           $1 million IRS phone scam 'largest ever'        F\n",
       "4  $1.2 B settlement with Toyota because of safet...        L"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.read_csv('./news_tagged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        News_Heading Risk Tag\n",
      "0   IMF chief Lagarde charged over French corrupt...        F\n",
      "1  $1 Billion Agreement with SunTrust to Address ...        L\n",
      "2                $1 million for GM victims' families        L\n",
      "3           $1 million IRS phone scam 'largest ever'        F\n",
      "4  $1.2 B settlement with Toyota because of safet...        L\n",
      "5  $1.2 Billion Settlement For Toyota's 2009-2010...        L\n",
      "6  $1.2 billion Toyota penalty sends warning to a...        L\n",
      "7  $1.27 Billion in Damages to be Paid by the Ban...        L\n",
      "8  $1.2B fine to settle probe delivers relief for...        L\n",
      "9                    $1.2B Toyota fine: GM's future?        L\n"
     ]
    }
   ],
   "source": [
    "sentences_df.head()\n",
    "print(sentences_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O    5154\n",
       "I    2190\n",
       "E    1376\n",
       "F    1249\n",
       "K     793\n",
       "G     528\n",
       "L     489\n",
       "N     480\n",
       "P     450\n",
       "S      84\n",
       "C      76\n",
       "T      54\n",
       "H      37\n",
       "A      25\n",
       "M      17\n",
       "Q       9\n",
       "Name: Risk Tag, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df['Risk Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News_Heading     IMF chief Lagarde charged over French corrupt...\n",
       "Risk Tag                                                        F\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning\n",
    "def sentence_cleaning(text):\n",
    "    sentence = text.lower()    \n",
    "    # clean and tokenize document string\n",
    "    sentence_content = sentence.split()    \n",
    "    word_list = []\n",
    "    for i in sentence_content:\n",
    "        x = 0\n",
    "        if (('http' not in i) and ('@' not in i) and ('<.*?>' not in i) and i.isalnum() and (not i in stop_words)):\n",
    "            word_list += [i]\n",
    "        \n",
    "    return word_list\n",
    "\n",
    "#Data Pre-processing\n",
    "def preprocessing(text):    \n",
    "    # remove numbers\n",
    "    number_tokens = [re.sub(r'[\\d]', ' ', i) for i in text]\n",
    "    number_tokens = ' '.join(number_tokens).split()\n",
    "     # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in number_tokens]\n",
    "    # remove empty\n",
    "    length_tokens = [i for i in stemmed_tokens if len(i) > 1]\n",
    "    return length_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences processed:  13011\n",
      "Number of non-empty sentence vectors:  13006\n"
     ]
    }
   ],
   "source": [
    "LabeledSentence1 = gensim.models.doc2vec.TaggedDocument\n",
    "all_content = []\n",
    "#texts = []\n",
    "j=0\n",
    "k=0\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "p_stemmer = PorterStemmer()\n",
    "for em in sentences_df.News_Heading:           \n",
    "    #Data cleaning\n",
    "    clean_content = sentence_cleaning(em)\n",
    "    \n",
    "    #Pre-processing\n",
    "    processed_sentences = preprocessing(clean_content)\n",
    "    #processed_sentences = clean_content\n",
    "    \n",
    "    # add tokens to list\n",
    "    if processed_sentences:\n",
    "        all_content.append(LabeledSentence1(processed_sentences,[k]))\n",
    "        j+=1\n",
    "        \n",
    "    k+=1\n",
    "\n",
    "print(\"Number of sentences processed: \", k)\n",
    "print(\"Number of non-empty sentence vectors: \", j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['imf', 'chief', 'lagard', 'charg', 'french', 'corrupt', 'case'], [0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_content[0])\n",
    "type(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec(vector_size = 50, window = 2, min_count = 1, workers=4, dm = 0, dbow_words = 1, negative = 15, alpha=0.025, min_alpha=0.0125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.build_vocab(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from random import shuffle\n",
    "    def randomize(sentence):\n",
    "        x = [sentence[i] for i in range(len(sentence))]\n",
    "        shuffle(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    d2v_model.train(randomize(all_content), total_examples=d2v_model.corpus_count, epochs=epoch, start_alpha=0.002, end_alpha=-0.016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaushlesh/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hit', 0.9999307990074158),\n",
       " ('chines', 0.9999301433563232),\n",
       " ('data', 0.9999286532402039),\n",
       " ('lead', 0.9999277591705322),\n",
       " ('dog', 0.9999269247055054),\n",
       " ('add', 0.9999266862869263),\n",
       " ('investig', 0.9999265670776367),\n",
       " ('food', 0.9999265074729919),\n",
       " ('comput', 0.9999237060546875),\n",
       " ('gm', 0.9999231100082397)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.most_similar('sold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chines', 'premier', 'zero', 'toler', 'corrupt']\n",
      "[-0.6340072   0.7821126   0.02652645  0.617036   -0.51429796 -0.59344846\n",
      " -0.3886683  -0.05824026  0.97966164  1.2516298  -0.7472035  -1.2175776\n",
      " -0.2687559  -0.82236177  0.6248389   0.2911443  -0.19019815 -0.31820032\n",
      " -0.8150261  -0.19659632  0.4825385   0.8837144  -0.64621097  0.34890807\n",
      " -0.01100883 -0.3017885  -1.5205872  -0.1889602  -0.27852216 -0.54767174\n",
      "  2.0140169  -0.01873509  0.2634091   0.05573512  1.0091954  -0.51070756\n",
      " -0.33269212 -0.19227661  0.43911564  0.10876785  0.13284042  0.10723174\n",
      "  0.59501755  0.65203077  0.08171245 -0.24769133 -0.6480989  -0.05637979\n",
      "  0.8334703   0.46032333]\n",
      "[12188.   558. 10849.  6571.   617. 10425.  5592. 11644. 12540.  7149.]\n",
      "12188\n",
      "TaggedDocument(['ico', 'probe', 'facebook', 'controversi', 'user', 'experi'], [12193])\n",
      "558\n",
      "TaggedDocument(['view', 'crowdfund', 'environment'], [559])\n",
      "10849\n",
      "TaggedDocument(['say', 'halifax', 'protest'], [10853])\n",
      "6571\n",
      "TaggedDocument(['nest', 'recal', 'smoke', 'alarm', 'safeti', 'risk'], [6574])\n",
      "617\n",
      "TaggedDocument(['surpris', 'bargain', 'stock'], [619])\n",
      "10425\n",
      "TaggedDocument(['recal', 'gm', 'vehicl'], [10428])\n",
      "5592\n",
      "TaggedDocument(['gm', 'recal', 'million', 'vehicl'], [5594])\n",
      "11644\n",
      "TaggedDocument(['state', 'attack', 'rise', 'cloud', 'threat'], [11648])\n",
      "12540\n",
      "TaggedDocument(['us', 'sanction', 'russia', 'would', 'equal'], [12545])\n",
      "7149\n",
      "TaggedDocument(['herbalif', 'share', 'plung', 'news', 'fbi', 'probe'], [7152])\n",
      "TaggedDocument(['view', 'crowdfund', 'environment'], [559])\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocessing(sentence_cleaning('Chinese Premier zero tolerance for corruption'))\n",
    "print(tokens)\n",
    "new_vector = d2v_model.infer_vector(tokens)\n",
    "print(new_vector)\n",
    "similar_doc = d2v_model.docvecs.most_similar(positive=[new_vector])\n",
    "similar_doc = np.array(similar_doc)\n",
    "print(similar_doc[:,0])\n",
    "for l in similar_doc[:,0]:\n",
    "    print(int(l))\n",
    "    print(all_content[int(l)])\n",
    "#print(all_content[558])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    }
   ],
   "source": [
    "#saving the created model\n",
    "d2v_model.save('d2v_model')\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.delete_temporary_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = Doc2Vec.load('d2v_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AffinityPropagation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AffinityPropagation(affinity='euclidean', convergence_iter=15, copy=True,\n",
       "          damping=0.5, max_iter=200, preference=None, verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AffinityPropagation(affinity='euclidean', convergence_iter=15, copy=True,\n",
       "          damping=0.5, max_iter=200, preference=None, verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering.fit(model_loaded.docvecs.vectors_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=clustering.labels_.tolist()\n",
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaushlesh/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `doctag_syn0` (Attribute will be removed in 4.0.0, use docvecs.vectors_docs instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.013\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(d2v_model.docvecs.doctag_syn0, labels, metric='sqeuclidean'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
